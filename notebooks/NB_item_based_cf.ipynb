{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python libraries\n",
    "import os\n",
    "import time\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "from typing import Any, Dict, List, Set, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Housekeeping ==> THIS GENERALLY STAYS THE SAME\n",
    "SEED, NUM_RECOMMENDATIONS = 183, 100\n",
    "task, task1_locales = 'task1', ['DE', 'JP', 'UK']\n",
    "NUM_CORES, NUM_THREADS = 8, 16\n",
    "data_path, output_path = '../../data/', '../../outputs/'\n",
    "train_path, test_path = data_path + 'train/', data_path + 'test/'\n",
    "output_file = output_path + task + '_predictions.parquet'\n",
    "project_files = ['products_train.csv', 'sessions_train.csv', f'sessions_test_{task}.csv']\n",
    "prod_dtypes = {\n",
    "    'id': 'object',\n",
    "    'locale': 'object',\n",
    "    'title': 'object',\n",
    "    'price': 'float64',\n",
    "    'brand': 'object',\n",
    "    'color': 'object',\n",
    "    'size': 'object',\n",
    "    'model': 'object',\n",
    "    'material': 'object',\n",
    "    'author': 'object',\n",
    "    'desc': 'object'\n",
    "}\n",
    "sess_dtypes = {\n",
    "    'session_id': 'int32'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "import data_processor\n",
    "importlib.reload(data_processor)\n",
    "from data_processor import handle_data\n",
    "\n",
    "import data_utils\n",
    "importlib.reload(data_utils)\n",
    "from data_utils import split_locales, split_locales_and_save, split_sessions, analyze_sessions, view_common_products\n",
    "\n",
    "import misc_utils\n",
    "importlib.reload(misc_utils)\n",
    "from misc_utils import clear_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA HERE ==> THIS GENERALLY STAYS THE SAME\n",
    "products_train, sessions_train, sessions_test = handle_data(\n",
    "    project_files, [train_path, train_path, test_path],\n",
    "    task, task1_locales, 1, SEED, prod_dtypes, sess_dtypes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_train.shape, sessions_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split datasets by locale\n",
    "p_train_de, p_train_jp, p_train_uk = split_locales(products_train, task1_locales)\n",
    "s_train_de, s_train_jp, s_train_uk = split_locales(sessions_train, task1_locales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train_de.shape, s_train_de.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice dataframes to get the first n rows\n",
    "slices = 1000\n",
    "p_train_de, s_train_de = p_train_de[:slices], s_train_de[:slices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train_de.shape, s_train_de.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "keep_vars = [p_train_de, s_train_de, sessions_test]\n",
    "clear_memory(keep_vars, globals_dict=globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sessions\n",
    "size_val = 0.15\n",
    "size_test = 0.15\n",
    "s_train_de, s_val_de, s_test_de = split_sessions(s_train_de, size_val, size_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_train_de.shape, s_val_de.shape, s_test_de.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item-based Collaborative Filtering\n",
    "Item-based collaborative filtering is a recommendation technique that uses the similarities between items to recommend similar products to users. For our case, item-based collaborative filtering can be used to recommend products based on their similarity in terms of features like product title, description, brand, or price. The intuition is that if a user engaged with a particular item, they are more likely to engage with similar items.\n",
    "\n",
    "<img src='../../img/cf.png' width=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 1**\n",
    "\n",
    "This is an item-based collaborative filtering model that combines product features to create a content-based representation of each product. The model computes item-item similarity using the cosine similarity of the TF-IDF vectors of the combined features. The resulting similarity matrix is then used to create a dictionary of item similarities, which can be used to recommend similar items to a given item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MODEL 1\n",
    "# def item_based_cf_model_1(\n",
    "#     products_train: dd.DataFrame, \n",
    "#     sessions_train: dd.DataFrame, \n",
    "#     hyperparams: Dict[str, Any], \n",
    "#     comb_features: List[str]\n",
    "# ) -> Dict[str, Dict[str, float]]:\n",
    "\n",
    "#     # Combine features from comb_features\n",
    "#     def combine_features(row: pd.Series, comb_features: List[str]) -> str:\n",
    "#         combined_features = \"\"\n",
    "#         for feature in comb_features:\n",
    "#             if feature in row and not pd.isnull(row[feature]):\n",
    "#                 combined_features += \" \" + row[feature]\n",
    "#         return combined_features.strip()\n",
    "\n",
    "#     # Compute item features\n",
    "#     products_train[\"combined_features\"] = products_train.apply(combine_features, axis=1, meta=('combined_features', 'object'), comb_features=comb_features)\n",
    "#     item_features = products_train[[\"id\", \"combined_features\"]].compute()\n",
    "\n",
    "#     # Compute item-item similarity matrix using the TF-IDF vectorizer\n",
    "#     vectorizer = TfidfVectorizer(\n",
    "#         min_df=hyperparams.get(\"min_df\", 2),\n",
    "#         max_df=hyperparams.get(\"max_df\", 0.8),\n",
    "#         ngram_range=hyperparams.get(\"ngram_range\", (1, 3))\n",
    "#     )\n",
    "#     combined_features_vectors = vectorizer.fit_transform(item_features[\"combined_features\"])\n",
    "#     similarity_matrix = cosine_similarity(combined_features_vectors)\n",
    "\n",
    "#     # Create item-item similarity dictionary\n",
    "#     item_similarity_dict = defaultdict(dict)\n",
    "#     for i in range(len(item_features)):\n",
    "#         for j in range(len(item_features)):\n",
    "#             item_id_1 = item_features.iloc[i][\"id\"]\n",
    "#             item_id_2 = item_features.iloc[j][\"id\"]\n",
    "#             item_similarity_dict[item_id_1][item_id_2] = similarity_matrix[i, j]\n",
    "\n",
    "#     return item_similarity_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 2**\n",
    "\n",
    "Model 2 is similar to Model 1 in that it combines product features to create a content-based representation of each product and computes item-item similarity using cosine similarity of the TF-IDF vectors. However, Model 2 introduces a key difference: it sorts the recommendations by similarity score and keeps only the top N most similar items for each item. This approach makes the recommendations more focused, keeping only the most relevant similar items for each given item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MODEL 2\n",
    "# def item_based_cf_model_2(\n",
    "#     products_train: dd.DataFrame, \n",
    "#     sessions_train: dd.DataFrame, \n",
    "#     hyperparams: Dict[str, Any], \n",
    "#     top_n: int\n",
    "# ) -> Dict[str, Dict[str, float]]:\n",
    "\n",
    "#     # Combine features from hyperparams['feat_combine']\n",
    "#     def combine_features(row: pd.Series) -> str:\n",
    "#         comb_features = hyperparams['feat_combine']\n",
    "#         combined_features = \"\"\n",
    "#         for feature in comb_features:\n",
    "#             if feature in row and not pd.isnull(row[feature]):\n",
    "#                 combined_features += \" \" + row[feature]\n",
    "#         return combined_features.strip()\n",
    "\n",
    "#     # Compute item features\n",
    "#     products_train[\"combined_features\"] = products_train.apply(combine_features, axis=1, meta=('combined_features', 'object'))\n",
    "#     item_features = products_train[[\"id\", \"combined_features\"]].compute()\n",
    "\n",
    "#     # Compute item-item similarity matrix using the TF-IDF vectorizer\n",
    "#     vectorizer = TfidfVectorizer(\n",
    "#         min_df=hyperparams.get(\"min_df\", 2),\n",
    "#         max_df=hyperparams.get(\"max_df\", 0.8),\n",
    "#         ngram_range=hyperparams.get(\"ngram_range\", (1, 3))\n",
    "#     )\n",
    "#     combined_features_vectors = vectorizer.fit_transform(item_features[\"combined_features\"])\n",
    "#     similarity_matrix = cosine_similarity(combined_features_vectors)\n",
    "\n",
    "#     # Create item-item similarity dictionary\n",
    "#     item_similarity_dict = defaultdict(dict)\n",
    "#     for i in range(len(item_features)):\n",
    "#         for j in range(len(item_features)):\n",
    "#             item_id_1 = item_features.iloc[i][\"id\"]\n",
    "#             item_id_2 = item_features.iloc[j][\"id\"]\n",
    "#             item_similarity_dict[item_id_1][item_id_2] = similarity_matrix[i, j]\n",
    "\n",
    "#         # Sort the recommendations by similarity score and take the top_n most similar items\n",
    "#         sorted_recommendations = sorted(item_similarity_dict[item_id_1].items(), key=lambda x: x[1], reverse=True)[:top_n + 1]\n",
    "#         item_similarity_dict[item_id_1] = dict(sorted_recommendations[1:])  # Exclude the first item (product itself)\n",
    "\n",
    "#     return item_similarity_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 11**\n",
    "\n",
    "Final improved version for item-based cf. Implemented batches to reduce memory footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the item similarity\n",
    "def save_item_similarity(item_similarity_dict: Dict[str, Dict[str, float]], output_path: str) -> None:\n",
    "    with open(output_path, \"a\") as f:\n",
    "        for item_id, similarity in item_similarity_dict.items():\n",
    "            f.write(f\"{item_id}:{','.join([f'{k}:{v}' for k, v in similarity.items()])}\\n\")\n",
    "\n",
    "# Function(s) to compute item similarity (incrementally)\n",
    "def compute_incremental_similarity(args: Tuple[int, str, np.ndarray, List[str], int]):\n",
    "    i, item_id_1, similarity_matrix, item_ids, top_n = args\n",
    "    sorted_idx = np.argsort(similarity_matrix[i, :])[-(top_n + 1):][::-1]\n",
    "    sorted_recommendations = {item_ids[j]: similarity_matrix[i, j] for j in sorted_idx if j != i}\n",
    "    return item_id_1, sorted_recommendations\n",
    "\n",
    "def compute_item_similarity_incremental(combined_features_vectors: np.ndarray, item_ids: List[str], top_n: int, batch_size: int, output_path: str = None):\n",
    "    n_items = combined_features_vectors.shape[0]\n",
    "    item_similarity_dict = defaultdict(dict)\n",
    "    total_batches = (n_items + batch_size - 1) // batch_size\n",
    "    batch_messages = []\n",
    "\n",
    "    for start_idx in range(0, n_items, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_items)\n",
    "        batch_similarity_matrix = cosine_similarity(combined_features_vectors[start_idx:end_idx], combined_features_vectors)\n",
    "        batch_item_ids = item_ids[start_idx:end_idx]\n",
    "\n",
    "        # Print batch info\n",
    "        batch_number = start_idx // batch_size + 1\n",
    "        batch_message = f\"Processing batch {batch_number} out of {total_batches}.\"\n",
    "        batch_messages.append(batch_message)\n",
    "\n",
    "        # Clear the output and display only the last 5 batch messages\n",
    "        clear_output(wait=True)\n",
    "        for msg in batch_messages[-5:]:\n",
    "            print(msg)\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            args = [(i, item_id_1, batch_similarity_matrix, item_ids, top_n) for i, item_id_1 in enumerate(batch_item_ids)]\n",
    "            results = executor.map(compute_incremental_similarity, args)\n",
    "\n",
    "            for item_id_1, sorted_recommendations in results:\n",
    "                item_similarity_dict[item_id_1] = sorted_recommendations\n",
    "            \n",
    "            if output_path:  # Save item similarity to the output file\n",
    "                save_item_similarity(item_similarity_dict, output_path)\n",
    "                item_similarity_dict.clear()  # Clear the current batch similarity from memory\n",
    "\n",
    "    return item_similarity_dict\n",
    "\n",
    "def item_based_cf_model_11(\n",
    "    products_train: pd.DataFrame, \n",
    "    sessions_train: pd.DataFrame, \n",
    "    hyperparams: Dict[str, Any], \n",
    "    top_n: int,\n",
    "    batch_size: int,\n",
    "    model_save: bool = False,\n",
    "    output_path: str = None\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    # Get unique products based on the 'incl_prod' parameter\n",
    "    def get_unique_products(\n",
    "        products_train: pd.DataFrame, \n",
    "        sessions_train: pd.DataFrame, \n",
    "        incl_prod: str\n",
    "    ) -> Set[str]:\n",
    "        if incl_prod == 'all':\n",
    "            core_item_set = set(products_train['id'].unique()) | set(pd.Series(sessions_train['prev_items'].str.split(',').sum()).unique())\n",
    "        elif incl_prod == 'prod_only':\n",
    "            core_item_set = set(products_train['id'].unique())\n",
    "        elif incl_prod == 'sess_only':\n",
    "            core_item_set = set(pd.Series(sessions_train['prev_items'].str.split(',').sum()).unique())\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value for 'incl_prod'. Choose from 'all', 'prod_only', or 'sess_only'.\")\n",
    "        return core_item_set\n",
    "\n",
    "    # Combine features from hyperparams['feat_combine']\n",
    "    def combine_features(row: pd.Series) -> str:\n",
    "        comb_features = hyperparams['feat_combine']\n",
    "        combined_features = \"\"\n",
    "        for feature in comb_features:\n",
    "            if feature in row and not pd.isnull(row[feature]):\n",
    "                combined_features += \" \" + row[feature]\n",
    "        return combined_features.strip()\n",
    "    \n",
    "    timer_1 = time.time()\n",
    "    core_item_set = get_unique_products(products_train, sessions_train, hyperparams['incl_prod'])\n",
    "    timer_2 = time.time()\n",
    "    print(f\"Time to get unique products: {timer_2 - timer_1:.2f} seconds.\")\n",
    "\n",
    "    if hyperparams['incl_prod'] == 'sess_only':\n",
    "        missing_product_ids = core_item_set.difference(products_train['id'].unique())\n",
    "        missing_products = pd.DataFrame({'id': list(missing_product_ids)})\n",
    "\n",
    "        missing_products['title'] = ''\n",
    "        missing_products['brand'] = ''\n",
    "        missing_products['color'] = ''\n",
    "        missing_products['size'] = ''\n",
    "        missing_products['model'] = ''\n",
    "        missing_products['material'] = ''\n",
    "        missing_products['author'] = ''\n",
    "        missing_products['desc'] = ''\n",
    "\n",
    "        products_train = pd.concat([products_train, missing_products], ignore_index=True)\n",
    "\n",
    "    products_train = products_train[products_train['id'].isin(core_item_set)]\n",
    "\n",
    "    timer_3 = time.time()\n",
    "    # Compute item features\n",
    "    products_train[\"combined_features\"] = products_train.apply(combine_features, axis=1)\n",
    "    item_features = products_train[[\"id\", \"combined_features\"]]\n",
    "    item_ids = item_features['id'].values\n",
    "    timer_4 = time.time()\n",
    "    print(f\"Time to compute item features: {timer_4 - timer_3:.2f} seconds.\")\n",
    "\n",
    "    if len(item_features) == 0:\n",
    "        print(\"Error: item_features is empty.\")\n",
    "        return {}\n",
    "\n",
    "    # Compute item-item similarity matrix using the TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        min_df=hyperparams.get(\"min_df\", 2),\n",
    "        max_df=hyperparams.get(\"max_df\", 0.8),\n",
    "        ngram_range=hyperparams.get(\"ngram_range\", (1, 3))\n",
    "    )\n",
    "    try:\n",
    "        timer_5 = time.time()\n",
    "        combined_features_vectors = vectorizer.fit_transform(item_features[\"combined_features\"])\n",
    "        timer_6 = time.time()\n",
    "        print(f\"Time to compute feature vectors with TF-IDF: {timer_6 - timer_5:.2f} seconds.\")\n",
    "    except ValueError:\n",
    "        print(\"Error: After pruning, no terms remain. Try a lower min_df or a higher max_df.\")\n",
    "        return {}\n",
    "    \n",
    "    timer_7 = time.time()\n",
    "    if not model_save:  # Only return actual item similarity if model_save is False (to help with debugging)\n",
    "        item_similarity_dict = compute_item_similarity_incremental(combined_features_vectors, item_ids, top_n, batch_size)\n",
    "    else:\n",
    "        if output_path is None:\n",
    "            raise ValueError(\"output_path must be provided when model_save is set to True\")\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "        with open(output_path, \"w\"): pass\n",
    "        compute_item_similarity_incremental(combined_features_vectors, item_ids, top_n, batch_size, output_path)\n",
    "        print(f\"Item Similarity saved to {output_path}\")\n",
    "        item_similarity_dict = {}  # Return an empty dictionary when model_save is True\n",
    "    timer_8 = time.time()\n",
    "    print(f\"Time to compute item similarity: {timer_8 - timer_7:.2f} seconds.\")\n",
    "\n",
    "    return item_similarity_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to quickly retrieve top n recommendations for a given product\n",
    "def view_recs(item_similarity_dict: Dict[str, Dict[str, float]], prod_to_rec: str, n: int) -> pd.DataFrame:\n",
    "    # Check if prod_to_rec is in the item_similarity_dict\n",
    "    if prod_to_rec not in item_similarity_dict:\n",
    "        print(f'{prod_to_rec} not found in the training data')\n",
    "        return\n",
    "\n",
    "    # Get the top n similar items\n",
    "    top_n_similar_items = sorted(item_similarity_dict[prod_to_rec].items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "    # Create a DataFrame to display the results\n",
    "    df_recommendations = pd.DataFrame(top_n_similar_items, columns=[\"related_products\", \"score\"])\n",
    "\n",
    "    return df_recommendations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Item Similarity Matrix (from JSON):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_item_similarity_json(file_path: str) -> Dict[str, Dict[str, float]]:\n",
    "    item_similarity_dict = defaultdict(dict)\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            item_id, similarities = line.strip().split(':', 1)\n",
    "            similarity_dict = {k: float(v) for k, v in (pair.split(':') for pair in similarities.split(','))}\n",
    "            item_similarity_dict[item_id] = similarity_dict\n",
    "\n",
    "    return item_similarity_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters for Model 1\n",
    "# feat_combine = ['title', 'brand', 'color', 'size', 'model', 'material', 'author', 'desc']\n",
    "# ibcf1_hyperparams = {\n",
    "#     'min_df': 5,                # Minimum number of documents a word must be present in to be kept\n",
    "#     'max_df': 0.8,              # Maximum % of documents a word can be present in to be kept\n",
    "#     'ngram_range': (1, 3)       # (min_n, max_n) the higher the n, the more computationally expensive\n",
    "#     }\n",
    "\n",
    "# # Specify product to get recommendations for\n",
    "# prod_to_rec = 'B005HIMQPW'\n",
    "\n",
    "# # Train model\n",
    "# start_time = time.time()\n",
    "# ibcf1_recos = item_based_cf_model_1(products_train, sessions_train, ibcf1_hyperparams, feat_combine)\n",
    "# end_time = time.time()\n",
    "# print(f'{end_time - start_time} seconds')\n",
    "\n",
    "# # View recommendations\n",
    "# ibcf1_recos_df = view_recs(ibcf1_recos, prod_to_rec, NUM_RECOMMENDATIONS)\n",
    "# print(f'Recommendations for {prod_to_rec}:')\n",
    "# ibcf1_recos_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters for Model 2\n",
    "# ibcf2_hyperparams = {\n",
    "#     'min_df': 5,                # Minimum number of documents a word must be present in to be kept\n",
    "#     'max_df': 0.8,              # Maximum % of documents a word can be present in to be kept\n",
    "#     'ngram_range': (1, 3),      # (min_n, max_n) the higher the n, the more computationally expensive\n",
    "#     'feat_combine': ['title', 'brand', 'color', 'size', 'model', 'material', 'author', 'desc']      # Combine this list of features\n",
    "# }\n",
    "\n",
    "# # Specify product to get recommendations for\n",
    "# prod_to_rec = 'B005HIMQPW'\n",
    "\n",
    "# # Train model\n",
    "# start_time = time.time()\n",
    "# ibcf2_recos = item_based_cf_model_2(products_train, sessions_train, ibcf2_hyperparams, NUM_RECOMMENDATIONS)\n",
    "# end_time = time.time()\n",
    "# print(f'{end_time - start_time} seconds')\n",
    "\n",
    "# # View recommendations\n",
    "# ibcf2_recos_df = view_recs(ibcf2_recos, prod_to_rec, NUM_RECOMMENDATIONS)\n",
    "# print(f'Recommendations for {prod_to_rec}:')\n",
    "# ibcf2_recos_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for Model 11\n",
    "ibcf11_hyperparams = {\n",
    "    'min_df': 5,                        # Minimum number of documents a word must be present in to be kept\n",
    "    'max_df': 0.8,                      # Maximum % of documents a word can be present in to be kept\n",
    "    'ngram_range': (1, 3),              # (min_n, max_n) the higher the n, the more computationally expensive\n",
    "    'feat_combine': ['title',           # Combine this list of features\n",
    "                     'brand',\n",
    "                     'color',\n",
    "                     'size',\n",
    "                     'model',\n",
    "                     'material',\n",
    "                     'author',\n",
    "                     'desc'],\n",
    "    'incl_prod': 'all'                  # Options: 'all', 'prod_only', 'sess_only'\n",
    "}\n",
    "batch_size = 500\n",
    "model_save = True\n",
    "file_ext = 'json'\n",
    "item_output = output_path + 'ibcf11_item_similarity' + '.' + file_ext\n",
    "\n",
    "# Train model\n",
    "start_time = time.time()\n",
    "if model_save == True:\n",
    "    item_based_cf_model_11(products_train, sessions_train, ibcf11_hyperparams, NUM_RECOMMENDATIONS, batch_size, model_save, item_output)\n",
    "    ibcf11_recos = load_item_similarity_json(item_output)\n",
    "else:\n",
    "    ibcf11_recos = item_based_cf_model_11(products_train, sessions_train, ibcf11_hyperparams, NUM_RECOMMENDATIONS, batch_size, model_save, item_output)\n",
    "end_time = time.time()\n",
    "print(f'{end_time - start_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify product to get recommendations for\n",
    "prod_to_rec = 'B06XKPB3GT'\n",
    "\n",
    "# View recommendations\n",
    "ibcf11_recos_df = view_recs(ibcf11_recos, prod_to_rec, NUM_RECOMMENDATIONS)\n",
    "print(f'Recommendations for {prod_to_rec}:')\n",
    "ibcf11_recos_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation: MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the Mean Reciprocal Rank (MRR)\n",
    "def calculate_mrr(item_similarity_dict: dict, test_data: pd.DataFrame, k: int) -> float:\n",
    "    reciprocal_ranks = []\n",
    "\n",
    "    for _, row in test_data.iterrows():\n",
    "        prev_items = row[\"prev_items\"].split(',')  # Extract the list of previous items\n",
    "        current_item = prev_items[-1]  # Get the last item in the list as the current item\n",
    "        next_item = row[\"next_item\"]\n",
    "\n",
    "        if current_item not in item_similarity_dict:\n",
    "            continue\n",
    "\n",
    "        sorted_similar_items = sorted(item_similarity_dict[current_item].items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        for rank, (item, _) in enumerate(sorted_similar_items[:k], start=1):\n",
    "            if item == next_item:\n",
    "                reciprocal_ranks.append(1 / rank)\n",
    "                break\n",
    "        else:\n",
    "            reciprocal_ranks.append(0)\n",
    "\n",
    "    mrr = sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0\n",
    "\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MRR for Model 11 (on generated test set)\n",
    "mrr_ibcf11 = calculate_mrr(ibcf11_recos, s_test_de, NUM_RECOMMENDATIONS)\n",
    "print(f'MRR for Model 11 (variant 1): {mrr_ibcf11}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate/Save Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate recommendations\n",
    "def generate_recommendations(\n",
    "    sessions_test: pd.DataFrame,\n",
    "    item_similarity_dict: Dict[str, Dict[str, float]],\n",
    "    top_n: int\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    recommendations = []\n",
    "\n",
    "    for _, row in sessions_test.iterrows():\n",
    "        session_id = row[\"session_id\"]\n",
    "        prev_items = row[\"prev_items\"].split(\",\")\n",
    "\n",
    "        session_recommendations = defaultdict(float)\n",
    "\n",
    "        for item in prev_items:\n",
    "            if item in item_similarity_dict:\n",
    "                for rec_item, score in item_similarity_dict[item].items():\n",
    "                    session_recommendations[rec_item] += score\n",
    "\n",
    "        sorted_recommendations = sorted(session_recommendations.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        recommendations.append((session_id, [rec[0] for rec in sorted_recommendations]))\n",
    "\n",
    "    recommendations_df = pd.DataFrame(recommendations, columns=[\"session_id\", \"next_item_prediction\"])\n",
    "\n",
    "    return recommendations_df\n",
    "\n",
    "\n",
    "# Function to save recommendations to parquet file\n",
    "def save_recs_to_pqt(recommendations_df: pd.DataFrame, output_file: str) -> None:\n",
    "    table = pa.Table.from_pandas(recommendations_df)\n",
    "    pq.write_table(table, output_file, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate recommendations on sessions_test using Model 11\n",
    "recommendations_df = generate_recommendations(sessions_test, ibcf11_recos, NUM_RECOMMENDATIONS)\n",
    "recommendations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the recommendations to a parquet file\n",
    "save_recs_to_pqt(recommendations_df, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tryfastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
