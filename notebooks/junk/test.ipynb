{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "\n",
    "# ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'task1'\n",
    "train_path = './data/train/'\n",
    "test_path = './data/test/sessions_test_' + task + '.csv'\n",
    "PREDS_PER_SESSION = 100\n",
    "slice_size = 1000\n",
    "\n",
    "# slicing data for memory management\n",
    "if slice_size != None:\n",
    "    train_prod = pd.read_csv(train_path + '/products_train.csv', nrows=slice_size)\n",
    "    train_sess = pd.read_csv(train_path + '/sessions_train.csv', nrows=slice_size)\n",
    "    test_sess = pd.read_csv(test_path, nrows=slice_size)\n",
    "else:\n",
    "    train_prod = pd.read_csv(train_path + '/products_train.csv')\n",
    "    train_sess = pd.read_csv(train_path + '/sessions_train.csv')\n",
    "    test_sess = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>locale</th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>brand</th>\n",
       "      <th>color</th>\n",
       "      <th>size</th>\n",
       "      <th>model</th>\n",
       "      <th>material</th>\n",
       "      <th>author</th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B005ZSSN10</td>\n",
       "      <td>DE</td>\n",
       "      <td>RED DRAGON Amberjack 3 - Steel Tip 22 Gramm Wo...</td>\n",
       "      <td>30.95</td>\n",
       "      <td>RED DRAGON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RDD0089</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Amberjacks Steel Dartpfeile sind verfügbar in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id locale                                              title  \\\n",
       "0  B005ZSSN10     DE  RED DRAGON Amberjack 3 - Steel Tip 22 Gramm Wo...   \n",
       "\n",
       "   price       brand color size    model material author  \\\n",
       "0  30.95  RED DRAGON   NaN  NaN  RDD0089      NaN    NaN   \n",
       "\n",
       "                                                desc  \n",
       "0  Amberjacks Steel Dartpfeile sind verfügbar in ...  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_prod.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 11 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   id        1000 non-null   object \n",
      " 1   locale    1000 non-null   object \n",
      " 2   title     1000 non-null   object \n",
      " 3   price     1000 non-null   float64\n",
      " 4   brand     981 non-null    object \n",
      " 5   color     748 non-null    object \n",
      " 6   size      572 non-null    object \n",
      " 7   model     467 non-null    object \n",
      " 8   material  576 non-null    object \n",
      " 9   author    49 non-null     object \n",
      " 10  desc      931 non-null    object \n",
      "dtypes: float64(1), object(10)\n",
      "memory usage: 86.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train_prod.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prev_items</th>\n",
       "      <th>next_item</th>\n",
       "      <th>locale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['B09W9FND7K' 'B09JSPLN1M']</td>\n",
       "      <td>B09M7GY217</td>\n",
       "      <td>DE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    prev_items   next_item locale\n",
       "0  ['B09W9FND7K' 'B09JSPLN1M']  B09M7GY217     DE"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sess.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   prev_items  1000 non-null   object\n",
      " 1   next_item   1000 non-null   object\n",
      " 2   locale      1000 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train_sess.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prev_items</th>\n",
       "      <th>locale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['B08V12CT4C' 'B08V1KXBQD' 'B01BVG1XJS' 'B09VC...</td>\n",
       "      <td>DE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          prev_items locale\n",
       "0  ['B08V12CT4C' 'B08V1KXBQD' 'B01BVG1XJS' 'B09VC...     DE"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sess.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   prev_items  1000 non-null   object\n",
      " 1   locale      1000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 15.8+ KB\n"
     ]
    }
   ],
   "source": [
    "test_sess.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "train_prod_txtcols = ['title', 'brand', 'color', 'size', 'model', 'material', 'author', 'desc']\n",
    "train_prod_subcols = train_prod_txtcols\n",
    "\n",
    "# Converting train_prod_subcols type to string\n",
    "for col in train_prod_subcols:\n",
    "    train_prod[col] = train_prod[col].astype(str)\n",
    "\n",
    "special_chars = set()\n",
    "for col in train_prod_subcols:\n",
    "    for text in train_prod[col]:\n",
    "        special_chars = special_chars.union(set(text))\n",
    "\n",
    "# Writing special_chars to a text file, each character on a new line and comma separated\n",
    "with open('special_chars.txt', 'w') as f:\n",
    "    for i, item in enumerate(special_chars):\n",
    "        if i == len(special_chars) - 1:\n",
    "            f.write(item)\n",
    "        else:\n",
    "            f.write(item + ',')\n",
    "    f.close()\n",
    "\n",
    "# Read special characters from file\n",
    "with open(\"special_chars.txt\", \"r\") as f:\n",
    "    special_chars = f.read().strip().split(',')\n",
    "\n",
    "# Categorize special characters\n",
    "categories = {}\n",
    "for char in special_chars:\n",
    "    if not char:\n",
    "        continue\n",
    "    category = unicodedata.category(char)\n",
    "    if category not in categories:\n",
    "        categories[category] = []\n",
    "    categories[category].append(char)\n",
    "\n",
    "# Output special characters by category to a json file\n",
    "import json\n",
    "\n",
    "with open(\"special_chars.json\", \"w\") as f:\n",
    "    json.dump(categories, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # showing row that has '$' or '#' or '@' or '%' or '|' in less_prod_text_cols\n",
    "# count1 = train_prod_new[train_prod_new['title'].str.contains('\\$|#|@|%|\\|', regex=True)]\n",
    "\n",
    "# # showing top 5 rows that have '$' in less_prod_text_cols\n",
    "# count2 = train_prod_new[train_prod_new['title'].str.contains('\\@', regex=True)].head()\n",
    "# print(count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# # Define a dictionary to map special characters to their textual value\n",
    "# special_char_mapping = {'@': 'at', '$': 'USD', '%': 'percent'}\n",
    "\n",
    "# # Iterate over each row and column in the DataFrame\n",
    "# for idx, row in train_prod_new.iterrows():\n",
    "#     for col in train_prod_new.columns:\n",
    "#         # Get the cell value\n",
    "#         cell_value = row[col]\n",
    "#         if isinstance(cell_value, str):\n",
    "#             # Check for special characters in the cell\n",
    "#             special_chars = re.findall(r'[^\\w\\s]', cell_value)\n",
    "#             for char in special_chars:\n",
    "#                 # Check if the special character is in the mapping dictionary\n",
    "#                 if char in special_char_mapping:\n",
    "#                     # Replace the special character with its textual value\n",
    "#                     cell_value = cell_value.replace(char, special_char_mapping[char])\n",
    "#                 else:\n",
    "#                     # Print the row, column, special character, and cell value for manual inspection\n",
    "#                     print(f'Row {idx}, Column {col}: Special character {char} found with value {cell_value}')\n",
    "#             # Update the cell value in the DataFrame\n",
    "#             train_prod_new.loc[idx, col] = cell_value\n",
    "\n",
    "# # showing top 5 rows that have '$' in less_prod_text_cols\n",
    "# count2 = train_prod_new[train_prod_new['title'].str.contains('\\@', regex=True)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# # Replace special characters\n",
    "# train_prod_new['title'] = train_prod_new['title'].str.replace('@', 'at')\n",
    "# train_prod_new['title'] = train_prod_new['title'].str.replace('$', 'USD')\n",
    "# train_prod_new['title'] = train_prod_new['title'].str.replace('%', 'percent')\n",
    "\n",
    "# # Identify other special characters\n",
    "# special_chars = set()\n",
    "# for title in train_prod_new['title']:\n",
    "#     special_chars.update(set(re.findall('[^A-Za-z0-9\\s]', title)))\n",
    "# special_chars\n",
    "\n",
    "# # printing special_chars to a text file\n",
    "# with open('special_chars.txt', 'w') as f:\n",
    "#     for item in special_chars:\n",
    "#         f.write(\"%s \" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data preprocessing\n",
    "# train_prod_new['title'] = train_prod_new['title'].str.lower()\n",
    "# train_prod_new['title'] = train_prod_new['title'].str.replace('[^a-z0-9 ]', '')\n",
    "# train_prod_new['title'] = train_prod_new['title'].str.replace(' +', ' ')\n",
    "# train_prod_new['title'] = train_prod_new['title'].str.strip()\n",
    "# train_prod_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # showing row index=54188\n",
    "# train_prod_new.iloc[1141]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textual_data = train_prod[['brand', 'color', 'size', 'model', 'material', 'author', 'desc']].values.flatten()\n",
    "# textual_data = [text.lower().replace(',', ' ').replace('.', ' ').replace(':', ' ').replace(';', ' ').replace('-', ' ').replace('/', ' ').replace('\"', ' ').replace(\"'\", ' ').replace('(', ' ').replace(')', ' ').replace('[', ' ').replace(']', ' ').replace('{', ' ').replace('}', ' ').replace('<', ' ').replace('>', ' ').replace('\\\\', ' ').replace('@', ' ').replace('#', ' ').replace('$', ' ').replace('%', ' ').replace('^', ' ').replace('&', ' ').replace('*', ' ').replace('+', ' ').replace('=', ' ').replace('!', ' ').replace('?', ' ').replace('~', ' ').replace('`', ' ').split() for text in textual_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# training a Word2Vec model on the textual columns\n",
    "corpus = [train_prod_new[col].astype(str).values.tolist() for col in train_prod_new.columns]\n",
    "corpus = [item for sublist in corpus for item in sublist]\n",
    "corpus = [item.split() for item in corpus]\n",
    "\n",
    "model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# # converting missing text values to vectors using Word2Vec\n",
    "# for col in train_prod_new.columns:\n",
    "#     col_vectors = []\n",
    "#     for text in train_prod_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train a Word2Vec model on the textual columns\n",
    "corpus = [train_prod[col].dropna().tolist() for col in ['brand', 'color', 'size', 'model', 'material', 'author', 'desc']]\n",
    "model = Word2Vec(corpus, vector_size=100, window=5, min_count=5, workers=4, sg=1)\n",
    "\n",
    "# Convert missing text values to vectors using Word2Vec\n",
    "for col in ['brand', 'color', 'size', 'model', 'material', 'author', 'desc']:\n",
    "    col_vectors = []\n",
    "    for text in train_prod[col].fillna('').tolist():\n",
    "        tokens = text.split()\n",
    "        vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "        if len(vectors) > 0:\n",
    "            col_vectors.append(np.mean(vectors, axis=0))\n",
    "        else:\n",
    "            col_vectors.append(np.zeros(100)) # Use a zero vector if all tokens are out-of-vocabulary\n",
    "    train_prod[col + '_vectors'] = col_vectors\n",
    "\n",
    "# Impute missing values using clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "for col in ['brand', 'color', 'size', 'model', 'material', 'author', 'desc']:\n",
    "    if train_prod[col].isna().sum() > 0:\n",
    "        X = np.stack(train_prod[col + '_vectors'])\n",
    "        kmeans = KMeans(n_clusters=10, random_state=0).fit(X)\n",
    "        cluster_assignments = kmeans.predict(X)\n",
    "        missing_mask = train_prod[col].isna()\n",
    "        missing_idx = np.where(missing_mask)[0]\n",
    "        train_prod.loc[missing_mask, col] = np.array([kmeans.cluster_centers_[i] for i in cluster_assignments[missing_mask]]).reshape(missing_idx.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the textual data\n",
    "textual_data = train_prod[['brand', 'color', 'size', 'model', 'material', 'author', 'desc']].values.flatten()\n",
    "textual_data = [text.lower().replace(',', ' ').replace('.', ' ').replace(':', ' ').replace(';', ' ').replace('-', ' ').replace('/', ' ').replace('\"', ' ').replace(\"'\", ' ').replace('(', ' ').replace(')', ' ').replace('[', ' ').replace(']', ' ').replace('{', ' ').replace('}', ' ').replace('<', ' ').replace('>', ' ').replace('\\\\', ' ').replace('@', ' ').replace('#', ' ').replace('$', ' ').replace('%', ' ').replace('^', ' ').replace('&', ' ').replace('*', ' ').replace('+', ' ').replace('=', ' ').replace('!', ' ').replace('?', ' ').replace('~', ' ').replace('`', ' ').split() for text in textual_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Word2Vec model\n",
    "model = Word2Vec(textual_data, size=100, min_count=1, window=5, workers=4)\n",
    "\n",
    "# Represent missing values as vectors\n",
    "for col in ['brand', 'color', 'size', 'model', 'material', 'author', 'desc']:\n",
    "    col_vectors = []\n",
    "    for text in train_prod[col]:\n",
    "        if pd.isna(text):\n",
    "            col_vectors.append(np.zeros(100)) # Use a zero vector for missing values\n",
    "        else:\n",
    "            tokens = text.lower().replace(',', ' ').replace('.', ' ').replace(':', ' ').replace(';', ' ').replace('-', ' ').replace('/', ' ').replace('\"', ' ').replace(\"'\", ' ').replace('(', ' ').replace(')', ' ').replace('[', ' ').replace(']', ' ').replace('{', ' ').replace('}', ' ').replace('<', ' ').replace('>', ' ').replace('\\\\', ' ').replace('@', ' ').replace('#', ' ').replace('$', ' ').replace('%', ' ').replace('^', ' ').replace('&', ' ').replace('*', ' ').replace('+', ' ').replace('=', ' ').replace('!', ' ').replace('?', ' ').replace('~', ' ').replace('`', ' ').split()\n",
    "            col_vectors.append(np.mean([model.wv[token] for token in tokens], axis=0))\n",
    "    train_prod[col] = col_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prod_nulls.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_prod_nulls.columns:\n",
    "    if col == 'id':\n",
    "        continue\n",
    "    else:\n",
    "        uniques = train_prod_nulls[col].unique()\n",
    "        for item in uniques:\n",
    "            print(col, item, train_prod_nulls[col].value_counts()[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_saver(df, path):\n",
    "    null = df[df.isnull().any(axis=1)]\n",
    "    if len(null) > 0:\n",
    "        print(f'Found {len(null)} null rows in {df.shape[0]} rows')\n",
    "        print(null.head())\n",
    "        # null.to_csv(path, index=False)\n",
    "        print(f'Saved {len(null)} null rows to {path}')\n",
    "    else:\n",
    "        print(f'No null rows found in {df}')\n",
    "\n",
    "null_saver(train_prod, './data/train/products_train-null.csv')\n",
    "null_saver(train_sess, './data/train/sessions_train-null.csv')\n",
    "null_saver(test_sess, './data/test/sessions_test_task1-null.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
