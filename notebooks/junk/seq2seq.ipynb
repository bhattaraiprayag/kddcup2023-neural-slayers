{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset for Task 1 locales\n",
    "locales_task1 = ['UK', 'DE', 'JP']\n",
    "sessions_train_task1 = sessions_train[sessions_train['locale'].isin(locales_task1)]\n",
    "\n",
    "\n",
    "# Merge product attributes with the training data\n",
    "sessions_train_task1 = sessions_train_task1.merge(products_train, left_on='next_item', right_on='id', how='left')\n",
    "\n",
    "\n",
    "# Tokenize product titles and descriptions\n",
    "sessions_train_task1['title_tokens'] = sessions_train_task1['title'].apply(lambda x: str(x).split())\n",
    "sessions_train_task1['desc_tokens'] = sessions_train_task1['desc'].apply(lambda x: str(x).split())\n",
    "\n",
    "\n",
    "# One-hot encode categorical features\n",
    "categorical_features = ['brand', 'color', 'size', 'model', 'material', 'author']\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded_features = encoder.fit_transform(sessions_train_task1[categorical_features].fillna(''))\n",
    "\n",
    "\n",
    "# Create a DataFrame with the encoded features and set the column names\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "\n",
    "# Normalize continuous features\n",
    "scaler = MinMaxScaler()\n",
    "sessions_train_task1['price_norm'] = scaler.fit_transform(sessions_train_task1[['price']])\n",
    "\n",
    "\n",
    "# Concatenate preprocessed features to the main DataFrame\n",
    "sessions_train_task1_preprocessed = pd.concat([sessions_train_task1, encoded_df], axis=1)\n",
    "\n",
    "\n",
    "# Define model parameters\n",
    "vocab_size = 10000                          # Set the size of your vocabulary (you may need to adjust this)\n",
    "embedding_dim = 128                         # Set the dimension of the word embeddings\n",
    "lstm_units = 128                            # Set the number of LSTM units\n",
    "num_encoded_features = encoded_df.shape[1]  # The number of one-hot encoded features\n",
    "max_seq_length = 20                         # Set the maximum length of input sequences (you may need to adjust this)\n",
    "\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_seq_length, ), name='encoder_inputs')\n",
    "encoder_embedding = Embedding(vocab_size, embedding_dim, mask_zero=True, name='encoder_embedding')(encoder_inputs)\n",
    "encoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True, name='encoder_lstm')\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_seq_length, ), name='decoder_inputs')\n",
    "decoder_embedding = Embedding(vocab_size, embedding_dim, mask_zero=True, name='decoder_embedding')(decoder_inputs)\n",
    "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "\n",
    "# Attention\n",
    "attention_layer = Attention(name='attention_layer')\n",
    "attention_output = attention_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "\n",
    "# Concatenate attention output with decoder output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attention_output])\n",
    "\n",
    "\n",
    "# Dense layer for predictions\n",
    "decoder_dense = Dense(vocab_size, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "\n",
    "# Tokenize the product titles\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(sessions_train_task1_preprocessed['prev_items'])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "\n",
    "# Convert the product titles to sequences\n",
    "input_sequences = tokenizer.texts_to_sequences(sessions_train_task1_preprocessed['prev_items'])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "\n",
    "# Prepare target sequences\n",
    "target_sequences = tokenizer.texts_to_sequences(sessions_train_task1_preprocessed['next_item'])\n",
    "target_sequences = pad_sequences(target_sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "\n",
    "# Convert target sequences to categorical\n",
    "target_sequences = np.expand_dims(target_sequences, axis=2)\n",
    "target_sequences = tf.keras.utils.to_categorical(target_sequences, num_classes=vocab_size)\n",
    "\n",
    "\n",
    "# Slice the dataset, for example, keep only 50% of the data\n",
    "input_sequences = input_sequences[: len(input_sequences) // 8]\n",
    "target_sequences = target_sequences[: len(target_sequences) // 8]\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(input_sequences, target_sequences, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Define model parameters\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "# Clear GPU memory\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "# Train the model\n",
    "checkpoint = ModelCheckpoint('seq2seq_attention_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "history = model.fit([X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, validation_data=([X_val, X_val], y_val), callbacks=[checkpoint])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
