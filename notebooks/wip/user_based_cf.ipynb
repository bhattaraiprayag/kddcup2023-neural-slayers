{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python libraries\n",
    "import re\n",
    "import time\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from dask import delayed\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Any, Dict, List, Set, Tuple\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "\n",
    "# Import project modules\n",
    "import kdd_processor\n",
    "importlib.reload(kdd_processor)\n",
    "from kdd_processor import handle_data\n",
    "\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Housekeeping ==> THIS GENERALLY STAYS THE SAME\n",
    "SEED, NUM_RECOMMENDATIONS = 183, 10\n",
    "task, task1_locales = 'task1', ['UK', 'DE', 'JP']\n",
    "NUM_CORES, NUM_THREADS = 8, 16\n",
    "data_path, output_path = '../../data/', '../../outputs/'\n",
    "train_path, test_path = data_path + 'train/', data_path + 'test/'\n",
    "output_file = output_path + task + '_predictions.parquet'\n",
    "project_files = ['products_train.csv', 'sessions_train.csv', f'sessions_test_{task}.csv']\n",
    "prod_dtypes = {\n",
    "    'id': 'object',\n",
    "    'locale': 'object',\n",
    "    'title': 'object',\n",
    "    'price': 'float64',\n",
    "    'brand': 'object',\n",
    "    'color': 'object',\n",
    "    'size': 'object',\n",
    "    'model': 'object',\n",
    "    'material': 'object',\n",
    "    'author': 'object',\n",
    "    'desc': 'object'\n",
    "}\n",
    "sess_dtypes = {\n",
    "    'session_id': 'int32'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed products file found at: ../../data/train/. Loading it now...\n",
      "Processed sessions file found at: ../../data/train/. Loading it now...\n",
      "Processed sessions test file found at: ../../data/test/. Loading it now...\n"
     ]
    }
   ],
   "source": [
    "# PARTITION SWITCH ==> CHANGE AS NEEDED\n",
    "# Specify partitions, ids and sampling fractions here\n",
    "num_partitions = 5000              # num. of subsets: None will automatically discard partition_ids below\n",
    "partition_ids = {                   # partition id(s): 'all', or any one of {integer = n, range = range(0, 6), list = [5, 19]} \n",
    "    'products_train': [0],\n",
    "    'sessions_train': [0],\n",
    "    'sessions_test': 'all'\n",
    "}\n",
    "fraction = 1                       # fraction of each partition to sample from, 1 will not sample\n",
    "\n",
    "\n",
    "# LOAD DATA HERE. This generally stays the same\n",
    "products_train, sessions_train, sessions_test = handle_data(\n",
    "    project_files,\n",
    "    [train_path, train_path, test_path],\n",
    "    task,\n",
    "    task1_locales,\n",
    "    num_partitions,\n",
    "    partition_ids,\n",
    "    fraction,\n",
    "    SEED,\n",
    "    prod_dtypes,\n",
    "    sess_dtypes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask\n",
    "We will be using Dask, as it is a great tool for parallel computing.\n",
    "\n",
    "Below are some examples. CAUTION: Dask dframes take time to compute(), uncomment as required\n",
    "\n",
    "Resources: https://www.youtube.com/@Dask-dev/videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # CAUTION: Dask dframes take time to compute, uncomment as required\n",
    "# products_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sessions_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sessions_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# products_train.compute().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sessions_train.compute().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sessions_test.compute().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# products_train.compute().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sessions_train.compute().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sessions_test.compute().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving details for a sample product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Details for a product\n",
    "# view_product = 'B06XKPB3GT'\n",
    "# products_train[products_train['id'] == view_product].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify product involvements\n",
    "We will be identifying products that are in both products_train and sessions_train. This will be USEFUL while getting recommendations later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting product occurrences took: 6.8472983837127686 seconds\n",
      "Retrieving common products took 2.519829511642456 seconds\n",
      "1 common products in products_train and sessions_train, for partitions: [0], [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>part_prod</th>\n",
       "      <th>row_prod</th>\n",
       "      <th>part_sess</th>\n",
       "      <th>row_sess</th>\n",
       "      <th>count_sess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B06XKPB3GT</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>209</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  part_prod  row_prod part_sess row_sess  count_sess\n",
       "0  B06XKPB3GT          0        13         0      209           1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to analyze dataframes\n",
    "def analyze_dataframes(\n",
    "    products_train: dd.DataFrame, \n",
    "    sessions_train: dd.DataFrame\n",
    ") -> Tuple[pd.Series, pd.Series, Dict[str, int], Dict[str, List[int]]]:\n",
    "    \n",
    "    # 1. Identify unique product ids in products_train\n",
    "    prod_in_pt = products_train['id'].unique().compute()\n",
    "\n",
    "    # 2. Identify unique product ids in sessions_train['prev_items']\n",
    "    sessions_train = sessions_train.assign(prev_items=sessions_train['prev_items'].str.split(','))\n",
    "    prod_in_st = sessions_train['prev_items'].explode().unique().compute()\n",
    "\n",
    "    # 3. Identify rows where each unique id in prod_in_pt can be found in products_train\n",
    "    products_train = products_train.reset_index().rename(columns={'index': 'row'})\n",
    "    prod_in_pt_rows = products_train[products_train['id'].isin(prod_in_pt)][['row', 'id']].compute().set_index('id')['row'].to_dict()\n",
    "\n",
    "    # 4. Identify rows where each unique id in prod_in_st occurs in sessions_train['prev_items'] and count occurrences\n",
    "    exploded_sessions_train = sessions_train.explode('prev_items').reset_index().rename(columns={'index': 'row'})\n",
    "    unique_pairs = exploded_sessions_train[['row', 'prev_items']].drop_duplicates()\n",
    "    prod_in_st_occs = unique_pairs[unique_pairs['prev_items'].isin(prod_in_st)].groupby('prev_items')['row'].apply(list, meta=('row', 'f8')).compute().to_dict()\n",
    "    \n",
    "    return prod_in_pt, prod_in_st, prod_in_pt_rows, prod_in_st_occs\n",
    "\n",
    "# Function to view common products\n",
    "def view_common_products(\n",
    "    products_train: dd.DataFrame,\n",
    "    sessions_train: dd.DataFrame,\n",
    "    prod_pt: pd.Series,\n",
    "    prod_st: pd.Series,\n",
    "    prod_pt_rows: Dict[str, int],\n",
    "    prod_st_occs: Dict[str, List[int]]\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # Calculate common product ids\n",
    "    common_ids = set(prod_pt) & set(prod_st)\n",
    "\n",
    "    # Function to calculate partition number\n",
    "    def get_partition(row_index, partition_size):\n",
    "        return row_index // partition_size\n",
    "\n",
    "    app_rows = []\n",
    "\n",
    "    # Calculate partition sizes\n",
    "    partition_size_products = len(products_train) // len(partition_ids['products_train'])\n",
    "    partition_size_sessions = len(sessions_train) // len(partition_ids['sessions_train'])\n",
    "\n",
    "    for common_id in common_ids:\n",
    "        count_in_sessions = len(prod_st_occs[common_id])\n",
    "        row_in_products = prod_pt_rows[common_id] + 1\n",
    "        app_data = prod_st_occs[common_id]\n",
    "        partition_numbers = [get_partition(x, partition_size_sessions) for x in app_data]\n",
    "\n",
    "        app_rows.append([common_id, None, row_in_products, \",\".join(map(str, partition_numbers)), \",\".join(map(str, [x + 1 for x in app_data])), count_in_sessions])\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    app_df = pd.DataFrame(app_rows, columns=['id', 'part_prod', 'row_prod', 'part_sess', 'row_sess', 'count_sess'])\n",
    "    app_df['part_prod'] = app_df['row_prod'].apply(lambda x: get_partition(x, partition_size_products))\n",
    "    app_df = app_df[['id', 'part_prod', 'row_prod', 'part_sess', 'row_sess', 'count_sess']]\n",
    "    app_df = app_df.sort_values(by=['row_prod']).reset_index(drop=True)\n",
    "    \n",
    "    return app_df\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Analyze dataframes\n",
    "prod_pt, prod_st, prod_pt_rows, prod_st_occs = analyze_dataframes(products_train, sessions_train)\n",
    "\n",
    "# Intermediate timer\n",
    "inter_time = time.time()\n",
    "print(f'Extracting product occurrences took: {inter_time - start_time} seconds')\n",
    "\n",
    "# View common products\n",
    "common_products = view_common_products(products_train, sessions_train, prod_pt, prod_st, prod_pt_rows, prod_st_occs)\n",
    "\n",
    "# End timer\n",
    "end_time = time.time()\n",
    "print(f'Retrieving common products took {end_time - inter_time} seconds')\n",
    "print(f'{len(common_products)} common products in products_train and sessions_train, for partitions: {partition_ids[\"products_train\"]}, {partition_ids[\"sessions_train\"]}')\n",
    "\n",
    "common_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>part_prod</th>\n",
       "      <th>row_prod</th>\n",
       "      <th>part_sess</th>\n",
       "      <th>row_sess</th>\n",
       "      <th>count_sess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, part_prod, row_prod, part_sess, row_sess, count_sess]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_products[common_products['count_sess'] > 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-based Collaborative Filtering\n",
    "User/session-based collaborative filtering is a recommendation technique that uses the similarities between items to recommend similar products to users. For our case, item-based collaborative filtering can be used to recommend products based on their similarity in terms of features like product title, description, brand, or price. The intuition is that if a user engaged with a particular item, they are more likely to engage with similar items.\n",
    "\n",
    "<img src='../../img/cf.png' width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1\n",
    "def SessionBasedCFModel1(products_train, sessions_train, hyperparams, top_n):\n",
    "    def create_interaction_matrix(sessions_train):\n",
    "        interactions = sessions_train.compute().groupby('session_id')['prev_items'].apply(lambda x: ','.join(x)).reset_index()\n",
    "        interaction_matrix = interactions['prev_items'].str.get_dummies(sep=',')\n",
    "        interaction_matrix.index = interactions['session_id']\n",
    "        return interaction_matrix\n",
    "\n",
    "    def calculate_similarity(interaction_matrix, metric):\n",
    "        similarity_matrix = 1 - squareform(pdist(interaction_matrix, metric))\n",
    "        similarity_df = pd.DataFrame(similarity_matrix, index=interaction_matrix.index, columns=interaction_matrix.index)\n",
    "        return similarity_df\n",
    "\n",
    "    def get_top_k_similar_sessions(similarity_df, k):\n",
    "        top_k_similar_sessions = similarity_df.apply(lambda x: x.nlargest(k + 1).iloc[1:], axis=1)\n",
    "        return top_k_similar_sessions\n",
    "\n",
    "    def get_recommendations(top_k_similar_sessions, interaction_matrix):\n",
    "        epsilon = 1e-9  # Small positive constant to avoid 0 values in similar_sessions\n",
    "        session_similarity_dict = defaultdict(dict)\n",
    "        for session_id, similar_sessions in top_k_similar_sessions.iterrows():\n",
    "            similar_sessions = similar_sessions + epsilon  # Add epsilon to similar_sessions\n",
    "            rec_items = interaction_matrix.loc[similar_sessions.index].apply(lambda x: np.dot(x, similar_sessions), axis=0)\n",
    "            sorted_recommendations = rec_items.sort_values(ascending=False).head(top_n + 1).index.tolist()\n",
    "            session_similarity_dict[session_id] = {item: rec_items[item] for item in sorted_recommendations if interaction_matrix.at[session_id, item] == 0}\n",
    "        return session_similarity_dict\n",
    "\n",
    "    interaction_matrix = create_interaction_matrix(sessions_train)\n",
    "    epsilon = 1e-9  # Small positive constant to avoid 0 values in interaction_matrix\n",
    "    interaction_matrix = interaction_matrix + epsilon  # Add epsilon to interaction_matrix\n",
    "    # Filter out sessions with no interactions\n",
    "    interaction_matrix = interaction_matrix.loc[(interaction_matrix != 0).any(axis=1)]\n",
    "    similarity_df = calculate_similarity(interaction_matrix, hyperparams['similarity_metric'])\n",
    "    top_k_similar_sessions = get_top_k_similar_sessions(similarity_df, hyperparams['top_k'])\n",
    "    session_similarity_dict = get_recommendations(top_k_similar_sessions, interaction_matrix)\n",
    "\n",
    "    return session_similarity_dict, interaction_matrix\n",
    "\n",
    "\n",
    "# Function to view recommendations\n",
    "def view_recs1(session_similarity_dict, interaction_matrix, prod_to_rec, n):\n",
    "    # Check if the product exists in the interaction_matrix\n",
    "    if prod_to_rec not in interaction_matrix.columns:\n",
    "        print(f'{prod_to_rec} not found in the training data')\n",
    "        return\n",
    "\n",
    "    # Find sessions that interacted with the given product\n",
    "    related_sessions = interaction_matrix[interaction_matrix[prod_to_rec] == 1].index\n",
    "\n",
    "    # If no session interacted with the given product\n",
    "    if len(related_sessions) == 0:\n",
    "        print(f'No sessions found with {prod_to_rec} in the training data')\n",
    "        return\n",
    "\n",
    "    # Aggregate recommendations from the related sessions\n",
    "    rec_dict = defaultdict(float)\n",
    "    for session_id in related_sessions:\n",
    "        recommendations = session_similarity_dict[session_id]\n",
    "        for item, score in recommendations.items():\n",
    "            rec_dict[item] += score\n",
    "\n",
    "    # Get the top n recommendations\n",
    "    top_n_recommendations = sorted(rec_dict.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "    # Create a DataFrame to display the results\n",
    "    df_recommendations = pd.DataFrame(top_n_recommendations, columns=[\"related_products\", \"score\"])\n",
    "\n",
    "    return df_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2\n",
    "def create_interaction_matrix(sessions_train):\n",
    "    interactions = sessions_train.compute().groupby('session_id')['prev_items'].apply(lambda x: ','.join(x)).reset_index()\n",
    "    interaction_matrix = interactions['prev_items'].str.get_dummies(sep=',')\n",
    "    interaction_matrix.index = interactions['session_id']\n",
    "    return csr_matrix(interaction_matrix)\n",
    "\n",
    "def calculate_similarity(interaction_matrix, metric):\n",
    "    similarity_matrix = 1 - pairwise_distances(interaction_matrix, metric=metric, n_jobs=-1)\n",
    "    return pd.DataFrame(similarity_matrix, index=interaction_matrix.index, columns=interaction_matrix.index)\n",
    "\n",
    "def get_top_k_similar_sessions(similarity_df, k):\n",
    "    top_k_indices = np.argpartition(similarity_df.values, -k-1, axis=1)[:, -k-1:-1]\n",
    "    top_k_similar_sessions = pd.DataFrame(top_k_indices, index=similarity_df.index, columns=[f\"Top{k+1}\" for k in range(k)])\n",
    "    return top_k_similar_sessions\n",
    "\n",
    "def get_recommendations(top_k_similar_sessions, interaction_matrix, top_n):\n",
    "    session_similarity_dict = defaultdict(dict)\n",
    "    for session_id, similar_sessions in top_k_similar_sessions.iterrows():\n",
    "        rec_items = interaction_matrix[similar_sessions.values].dot(interaction_matrix[session_id].T).toarray().sum(axis=0)\n",
    "        top_item_indices = np.argpartition(rec_items, -top_n-1)[-top_n-1:-1]\n",
    "        session_similarity_dict[session_id] = {item: rec_items[item] for item in top_item_indices if interaction_matrix[session_id, item] == 0}\n",
    "    return session_similarity_dict\n",
    "\n",
    "def SessionBasedCFModel2(products_train, sessions_train, hyperparams, top_n):\n",
    "    interaction_matrix = create_interaction_matrix(sessions_train)\n",
    "    similarity_df = calculate_similarity(interaction_matrix, hyperparams['similarity_metric'])\n",
    "    top_k_similar_sessions = get_top_k_similar_sessions(similarity_df, hyperparams['top_k'])\n",
    "    session_similarity_dict = get_recommendations(top_k_similar_sessions, interaction_matrix, top_n)\n",
    "    return session_similarity_dict, interaction_matrix\n",
    "\n",
    "\n",
    "# Function to view recommendations\n",
    "def view_recs2(session_similarity_dict, interaction_matrix, prod_to_rec, n):\n",
    "    related_sessions = interaction_matrix[:, interaction_matrix.columns.get_loc(prod_to_rec)].nonzero()[0]\n",
    "\n",
    "    if len(related_sessions) == 0:\n",
    "        print(f'{prod_to_rec} not found in the training data')\n",
    "        return\n",
    "\n",
    "    rec_dict = defaultdict(float)\n",
    "    for session_id in related_sessions:\n",
    "        recommendations = session_similarity_dict[session_id]\n",
    "        for item, score in recommendations.items():\n",
    "            rec_dict[item] += score\n",
    "\n",
    "    top_n_recommendations = sorted(rec_dict.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "    df_recommendations = pd.DataFrame(top_n_recommendations, columns=[\"related_products\", \"score\"])\n",
    "    return df_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sessions found with B06XKPB3GT in the training data\n",
      "Recommendations for B06XKPB3GT:\n"
     ]
    }
   ],
   "source": [
    "# Hyperparams for SessionBasedCFModel\n",
    "sbcf_hyperparams = {\n",
    "    'similarity_metric': 'cosine',  # or 'jaccard', 'correlation', etc.\n",
    "    'top_k': 5\n",
    "}\n",
    "\n",
    "\n",
    "# Specify product to get recommendations for\n",
    "prod_to_rec = 'B06XKPB3GT'\n",
    "\n",
    "\n",
    "# Train and predict\n",
    "sbcf_similarity, sbcf_interaction_matrix = SessionBasedCFModel1(products_train, sessions_train, sbcf_hyperparams, NUM_RECOMMENDATIONS)\n",
    "\n",
    "\n",
    "# View recommendations\n",
    "sbcf_recos_df = view_recs1(sbcf_similarity, sbcf_interaction_matrix, prod_to_rec, NUM_RECOMMENDATIONS)\n",
    "print(f'Recommendations for {prod_to_rec}:')\n",
    "sbcf_recos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tryfastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
